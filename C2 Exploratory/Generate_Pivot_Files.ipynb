{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total time: 108.548623085\n"
     ]
    }
   ],
   "source": [
    "# This cell assumes you hav the five_min_frame.hdf which is a hdf output of a pandas dataframe\n",
    "# of all 2015 data without the lane specific columns\n",
    "# start_time = time.time()\n",
    "# df = pd.read_hdf('five_min_frame.hdf','five_min_frame')\n",
    "# print \"total time: %s\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# filter to main line stations only\n",
    "# df_ml = df[df['Lane Type'] == 'ML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_core = df_ml[['Timestamp', 'Station', 'Freeway #', 'Direction', '% Observed', 'Total Flow', 'Avg Occupancy',\n",
    "#                  'Avg Speed']]\n",
    "# df_core.to_hdf('2015_ml_core.hdf', 'ml_core', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If you have the 2015_ml_core.hdf file then start here\n",
    "df_core = pd.read_hdf('2015_ml_core.hdf', 'ml_core')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "no_dup_keep_last = pd.read_hdf('meta_2015.hdf', 'meta_2015')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "to_loop = no_dup_keep_last.groupby(['Fwy', 'Dir'])['ID'].count().reset_index()[['Fwy', 'Dir']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "daymap = {0:'Monday', 1:'Tuesday', 2:'Wednesday', 3:'Thursday', 4:'Friday', 5:'Saturday', 6:'Sunday'}\n",
    "\n",
    "def add_date_time_weekday( df ):\n",
    "    \"\"\"\n",
    "    Josh's add date_time function\n",
    "    \"\"\"\n",
    "    df.loc[:,'Timestamp2'] = pd.to_datetime(df['Timestamp'], format='%m/%d/%Y %H:%M:%S')\n",
    "    df.loc[:,'Time'] = df['Timestamp2'].dt.time\n",
    "    df.loc[:,'Date'] = df['Timestamp2'].dt.date\n",
    "    df.loc[:,'Weekday'] = df['Date'].apply( lambda x: daymap[x.weekday()] )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_freeway_day_pivot(Fwy, Dir):\n",
    "    \"\"\"\n",
    "    This function is designed to pivot a particular freeway / direction\n",
    "    \"\"\"\n",
    "    print \"pivoting Fwy: %s Dir: %s\" % (Fwy, Dir)\n",
    "    weekdays = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "    weekends = ['Saturday', 'Sunday']\n",
    "    \n",
    "    if Dir == \"N\":\n",
    "        sort_order = ('Abs_PM', True)        \n",
    "    elif Dir == \"S\":\n",
    "        sort_order = ('Abs_PM', False)        \n",
    "    elif Dir == \"E\":\n",
    "        sort_order = ('Abs_PM', True)        \n",
    "    elif Dir == \"W\":\n",
    "        sort_order = ('Abs_PM', False) \n",
    "\n",
    "    ret = no_dup_keep_last[(no_dup_keep_last.Fwy == Fwy) & (no_dup_keep_last.Dir == Dir)]\\\n",
    "        .sort_values(by=sort_order[0], ascending=sort_order[1])\n",
    "    ret.index = np.arange(0, ret.shape[0])\n",
    "    station_list = list(ret.ID.values)\n",
    "    stations = df_core.Station.isin(station_list)\n",
    "    df_stations = df_core[stations]\n",
    "\n",
    "    if df_stations.Timestamp.count() > 0:\n",
    "        for col, f_name in [('Total Flow', 'Flow'), ('Avg Occupancy', 'Occu'), ('Avg Speed', 'Speed')]:\n",
    "    #         for day in daymap.values():\n",
    "    #             # print \"day: %s\" % day\n",
    "    #             df = stations_with_date[stations_with_date.Weekday == day]\n",
    "\n",
    "    #             # note: only creating flow at this point...\n",
    "    #             pivoted = df.pivot_table(index='Station', columns='Time', values=col, aggfunc=np.mean)\n",
    "    #             name = '%s_2015_%s_%s_%s' % (f_name, Fwy, Dir, day)\n",
    "    #             pivoted.to_hdf('%s.hdf' % (name), name, mode='w')\n",
    "\n",
    "            for partition, part_name in [(weekdays, 'Weekdays'), (weekends, 'Weekends')]:\n",
    "                # print \"part_name: %s\" % part_name\n",
    "                partition_df = stations_with_date[ stations_with_date.Weekday.isin(partition) ]\n",
    "                pivoted = partition_df.pivot_table(index='Station', columns='Time', values=col, aggfunc=np.mean)\n",
    "                name = '%s_2015_%s_%s_%s' % (f_name, Fwy, Dir, part_name)\n",
    "                pivoted.to_hdf('%s.hdf' % (name), name, mode='w')\n",
    "    else:\n",
    "        print \"Fwy: %s Dir: %s has no data\" % (Fwy, Dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pivoting Fwy: 5 Dir: N\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python2.7/site-packages/pandas/core/indexing.py:288: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "//anaconda/lib/python2.7/site-packages/pandas/core/indexing.py:465: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n",
      "//anaconda/lib/python2.7/site-packages/pandas/io/pytables.py:260: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->time,key->axis0] [items->None]\n",
      "\n",
      "  f(store)\n",
      "//anaconda/lib/python2.7/site-packages/pandas/io/pytables.py:260: PerformanceWarning: \n",
      "your performance may suffer as PyTables will pickle object types that it cannot\n",
      "map directly to c-types [inferred_type->time,key->block0_items] [items->None]\n",
      "\n",
      "  f(store)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 293.240375996\n",
      "pivoting Fwy: 5 Dir: S\n",
      "time: 47.0048408508\n",
      "pivoting Fwy: 8 Dir: E\n",
      "time: 44.1970670223\n",
      "pivoting Fwy: 8 Dir: W\n",
      "time: 42.310585022\n",
      "pivoting Fwy: 15 Dir: N\n",
      "time: 43.1920349598\n",
      "pivoting Fwy: 15 Dir: S\n",
      "time: 43.387130022\n",
      "pivoting Fwy: 52 Dir: E\n",
      "time: 45.5881698132\n",
      "pivoting Fwy: 52 Dir: W\n",
      "time: 42.3144989014\n",
      "pivoting Fwy: 54 Dir: E\n",
      "time: 44.0053391457\n",
      "pivoting Fwy: 54 Dir: W\n",
      "time: 47.2775499821\n",
      "pivoting Fwy: 56 Dir: E\n",
      "time: 41.1542019844\n",
      "pivoting Fwy: 56 Dir: W\n",
      "time: 42.6294419765\n",
      "pivoting Fwy: 67 Dir: N\n",
      "time: 45.3795349598\n",
      "pivoting Fwy: 67 Dir: S\n",
      "time: 44.1995658875\n",
      "pivoting Fwy: 78 Dir: E\n",
      "time: 39.8686261177\n",
      "pivoting Fwy: 78 Dir: W\n",
      "time: 42.1908481121\n",
      "pivoting Fwy: 94 Dir: E\n",
      "time: 43.8916831017\n",
      "pivoting Fwy: 94 Dir: W\n",
      "time: 43.0892989635\n",
      "pivoting Fwy: 125 Dir: N\n",
      "time: 40.7641320229\n",
      "pivoting Fwy: 125 Dir: S\n",
      "time: 43.0152680874\n",
      "pivoting Fwy: 163 Dir: N\n",
      "time: 49.2900438309\n",
      "pivoting Fwy: 163 Dir: S\n",
      "time: 39.2246778011\n",
      "pivoting Fwy: 805 Dir: N\n",
      "time: 41.6067929268\n",
      "pivoting Fwy: 805 Dir: S\n",
      "time: 44.0112011433\n",
      "pivoting Fwy: 905 Dir: E\n",
      "time: 48.3214151859\n",
      "pivoting Fwy: 905 Dir: W\n",
      "time: 39.1938791275\n"
     ]
    }
   ],
   "source": [
    "for Fwy, Dir in to_loop:\n",
    "    start_time = time.time()\n",
    "    stations_with_date = create_freeway_day_pivot(Fwy, Dir)\n",
    "    print \"time: %s\" % (time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow_2015_125_N_Weekdays.hdf Flow_2015_5_S_Weekdays.hdf\r\n",
      "Flow_2015_125_N_Weekends.hdf Flow_2015_5_S_Weekends.hdf\r\n",
      "Flow_2015_125_S_Weekdays.hdf Flow_2015_67_N_Weekdays.hdf\r\n",
      "Flow_2015_125_S_Weekends.hdf Flow_2015_67_N_Weekends.hdf\r\n",
      "Flow_2015_15_N_Weekdays.hdf  Flow_2015_67_S_Weekdays.hdf\r\n",
      "Flow_2015_15_N_Weekends.hdf  Flow_2015_67_S_Weekends.hdf\r\n",
      "Flow_2015_15_S_Weekdays.hdf  Flow_2015_78_E_Weekdays.hdf\r\n",
      "Flow_2015_15_S_Weekends.hdf  Flow_2015_78_E_Weekends.hdf\r\n",
      "Flow_2015_163_N_Weekdays.hdf Flow_2015_78_W_Weekdays.hdf\r\n",
      "Flow_2015_163_N_Weekends.hdf Flow_2015_78_W_Weekends.hdf\r\n",
      "Flow_2015_163_S_Weekdays.hdf Flow_2015_805_N_Weekdays.hdf\r\n",
      "Flow_2015_163_S_Weekends.hdf Flow_2015_805_N_Weekends.hdf\r\n",
      "Flow_2015_52_E_Weekdays.hdf  Flow_2015_805_S_Weekdays.hdf\r\n",
      "Flow_2015_52_E_Weekends.hdf  Flow_2015_805_S_Weekends.hdf\r\n",
      "Flow_2015_52_W_Weekdays.hdf  Flow_2015_8_E_Weekdays.hdf\r\n",
      "Flow_2015_52_W_Weekends.hdf  Flow_2015_8_E_Weekends.hdf\r\n",
      "Flow_2015_54_E_Weekdays.hdf  Flow_2015_8_W_Weekdays.hdf\r\n",
      "Flow_2015_54_E_Weekends.hdf  Flow_2015_8_W_Weekends.hdf\r\n",
      "Flow_2015_54_W_Weekdays.hdf  Flow_2015_905_E_Weekdays.hdf\r\n",
      "Flow_2015_54_W_Weekends.hdf  Flow_2015_905_E_Weekends.hdf\r\n",
      "Flow_2015_56_E_Weekdays.hdf  Flow_2015_905_W_Weekdays.hdf\r\n",
      "Flow_2015_56_E_Weekends.hdf  Flow_2015_905_W_Weekends.hdf\r\n",
      "Flow_2015_56_W_Weekdays.hdf  Flow_2015_94_E_Weekdays.hdf\r\n",
      "Flow_2015_56_W_Weekends.hdf  Flow_2015_94_E_Weekends.hdf\r\n",
      "Flow_2015_5_N_Weekdays.hdf   Flow_2015_94_W_Weekdays.hdf\r\n",
      "Flow_2015_5_N_Weekends.hdf   Flow_2015_94_W_Weekends.hdf\r\n"
     ]
    }
   ],
   "source": [
    "# List freeway weekday / weekends\n",
    "!ls Flow_2015*Week*.hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Occu_2015_125_N_Weekdays.hdf Occu_2015_5_S_Weekdays.hdf\r\n",
      "Occu_2015_125_N_Weekends.hdf Occu_2015_5_S_Weekends.hdf\r\n",
      "Occu_2015_125_S_Weekdays.hdf Occu_2015_78_E_Weekdays.hdf\r\n",
      "Occu_2015_125_S_Weekends.hdf Occu_2015_78_E_Weekends.hdf\r\n",
      "Occu_2015_15_N_Weekdays.hdf  Occu_2015_78_W_Weekdays.hdf\r\n",
      "Occu_2015_15_N_Weekends.hdf  Occu_2015_78_W_Weekends.hdf\r\n",
      "Occu_2015_15_S_Weekdays.hdf  Occu_2015_805_N_Weekdays.hdf\r\n",
      "Occu_2015_15_S_Weekends.hdf  Occu_2015_805_N_Weekends.hdf\r\n",
      "Occu_2015_163_N_Weekdays.hdf Occu_2015_805_S_Weekdays.hdf\r\n",
      "Occu_2015_163_N_Weekends.hdf Occu_2015_805_S_Weekends.hdf\r\n",
      "Occu_2015_163_S_Weekdays.hdf Occu_2015_8_E_Weekdays.hdf\r\n",
      "Occu_2015_163_S_Weekends.hdf Occu_2015_8_E_Weekends.hdf\r\n",
      "Occu_2015_52_E_Weekdays.hdf  Occu_2015_8_W_Weekdays.hdf\r\n",
      "Occu_2015_52_E_Weekends.hdf  Occu_2015_8_W_Weekends.hdf\r\n",
      "Occu_2015_52_W_Weekdays.hdf  Occu_2015_905_E_Weekdays.hdf\r\n",
      "Occu_2015_52_W_Weekends.hdf  Occu_2015_905_E_Weekends.hdf\r\n",
      "Occu_2015_54_E_Weekdays.hdf  Occu_2015_905_W_Weekdays.hdf\r\n",
      "Occu_2015_54_E_Weekends.hdf  Occu_2015_905_W_Weekends.hdf\r\n",
      "Occu_2015_54_W_Weekdays.hdf  Occu_2015_94_E_Weekdays.hdf\r\n",
      "Occu_2015_54_W_Weekends.hdf  Occu_2015_94_E_Weekends.hdf\r\n",
      "Occu_2015_56_E_Weekdays.hdf  Occu_2015_94_W_Weekdays.hdf\r\n",
      "Occu_2015_56_E_Weekends.hdf  Occu_2015_94_W_Weekends.hdf\r\n",
      "Occu_2015_56_W_Weekdays.hdf  Occu_E_2015.hdf\r\n",
      "Occu_2015_56_W_Weekends.hdf  Occu_N_2015.hdf\r\n",
      "Occu_2015_5_N_Weekdays.hdf   Occu_S_2015.hdf\r\n",
      "Occu_2015_5_N_Weekends.hdf   Occu_W_2015.hdf\r\n"
     ]
    }
   ],
   "source": [
    "!ls Occ*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create average year dataframes per direction\n",
    "# Note: for some reason doing the time calculation on the entire year at once never finished, but\n",
    "# breaking it into directions of N, S, E, W and then doing it worked\n",
    "for Dir in ['N', 'S', 'E', 'W']:\n",
    "    sub_frame = df_core[df_core.Direction == Dir]\n",
    "    new_time = pd.to_datetime(sub_frame['Timestamp'], format='%m/%d/%Y %H:%M:%S').dt.time\n",
    "    sub_frame['Time'] = new_time\n",
    "    for col, f_name in [('Total Flow', 'Flow'), ('Avg Occupancy', 'Occu'), ('Avg Speed', 'Speed')]:\n",
    "        pivoted = sub_frame.pivot_table(index='Station', columns='Time', values=col, aggfunc=np.mean)\n",
    "        name = '%s_%s_2015' % (f_name, Dir)\n",
    "        pivoted.to_hdf('%s.hdf' % (name), name, mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct yearly average Flow Speed and Occu vectors from the N, S, E, W\n",
    "for data_type in ['Flow', 'Speed', 'Occu']:\n",
    "    files = !ls Flow_*_2015.hdf\n",
    "    frames = []\n",
    "    for item in files:\n",
    "        name = item.split('.')[0]\n",
    "        frames.append(pd.read_hdf(item, name))\n",
    "    total = pd.concat(frames)\n",
    "    total_mean = total.mean()\n",
    "    total_mean.to_hdf('%s_2015.hdf' % data_type, '%s_2015' % data_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow_2015.hdf                Flow_2015_5_S_Weekdays.hdf\r\n",
      "Flow_2015_125_N_Weekdays.hdf Flow_2015_5_S_Weekends.hdf\r\n",
      "Flow_2015_125_N_Weekends.hdf Flow_2015_78_E_Weekdays.hdf\r\n",
      "Flow_2015_125_S_Weekdays.hdf Flow_2015_78_E_Weekends.hdf\r\n",
      "Flow_2015_125_S_Weekends.hdf Flow_2015_78_W_Weekdays.hdf\r\n",
      "Flow_2015_15_N_Weekdays.hdf  Flow_2015_78_W_Weekends.hdf\r\n",
      "Flow_2015_15_N_Weekends.hdf  Flow_2015_805_N_Weekdays.hdf\r\n",
      "Flow_2015_15_S_Weekdays.hdf  Flow_2015_805_N_Weekends.hdf\r\n",
      "Flow_2015_15_S_Weekends.hdf  Flow_2015_805_S_Weekdays.hdf\r\n",
      "Flow_2015_163_N_Weekdays.hdf Flow_2015_805_S_Weekends.hdf\r\n",
      "Flow_2015_163_N_Weekends.hdf Flow_2015_8_E_Weekdays.hdf\r\n",
      "Flow_2015_163_S_Weekdays.hdf Flow_2015_8_E_Weekends.hdf\r\n",
      "Flow_2015_163_S_Weekends.hdf Flow_2015_8_W_Weekdays.hdf\r\n",
      "Flow_2015_52_E_Weekdays.hdf  Flow_2015_8_W_Weekends.hdf\r\n",
      "Flow_2015_52_E_Weekends.hdf  Flow_2015_905_E_Weekdays.hdf\r\n",
      "Flow_2015_52_W_Weekdays.hdf  Flow_2015_905_E_Weekends.hdf\r\n",
      "Flow_2015_52_W_Weekends.hdf  Flow_2015_905_W_Weekdays.hdf\r\n",
      "Flow_2015_54_E_Weekdays.hdf  Flow_2015_905_W_Weekends.hdf\r\n",
      "Flow_2015_54_E_Weekends.hdf  Flow_2015_94_E_Weekdays.hdf\r\n",
      "Flow_2015_54_W_Weekdays.hdf  Flow_2015_94_E_Weekends.hdf\r\n",
      "Flow_2015_54_W_Weekends.hdf  Flow_2015_94_W_Weekdays.hdf\r\n",
      "Flow_2015_56_E_Weekdays.hdf  Flow_2015_94_W_Weekends.hdf\r\n",
      "Flow_2015_56_E_Weekends.hdf  Flow_E_2015.hdf\r\n",
      "Flow_2015_56_W_Weekdays.hdf  Flow_N_2015.hdf\r\n",
      "Flow_2015_56_W_Weekends.hdf  Flow_S_2015.hdf\r\n",
      "Flow_2015_5_N_Weekdays.hdf   Flow_W_2015.hdf\r\n",
      "Flow_2015_5_N_Weekends.hdf\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
