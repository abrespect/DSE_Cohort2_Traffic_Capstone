{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic prediction model\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This notebook explores a method to predicting traffic patterns on freeways. A naive approach to predict traffic for a particular station would be to use traffic state from all other station on a freeway. This would prove to be computatinally heavy and not feasable for a notebook. However based on this theme there are many simpler permutation of station combinations that can be fed into the model. For example; State of all stations. State of all stations in a freeway. State of all preceding stations. State of preceding staiton. State of all postceding station. State of postceding station. State of post and precdeding station, etc.\n",
    "\n",
    "For this model the assumtion is made that cars do appear or disapear on any segment of freeway between two stations, execpt when there are intermediate onramps or offramps between stations. With this assumption, a prediction should be possible based on the preceding stations flow and occupancy. This is the model described below. \n",
    "\n",
    "This method should capture the intial traffic formation. This method may not capture the observed traffic propigation. Specifically backwards traveling waves of traffic. Further refinement of the model may include adding the state of postceding stations. \n",
    "\n",
    "This model consists of two submodels. The first model called the Station Traffic Model (STM) predicts traffic patterns for an individual Station Model (SM) based on the preceding station's flow, occupancy, and intermediate on ramp and off ramp flows. The second model called the Freeway Traffic Model (FTM) predicts traffic patterns for a freeway from the SMs, and previous station state. The model is intialized with initial station state. The model is used by feeding in previously predicted station state and pumping the first staiton flow and occupancy, and on-ramps and off-ramp flows.\n",
    "\n",
    "Once traffic has been predicted use the fundumental diagram  based model get a predction of confidence of a stations traffic state for each time step in the FTM\n",
    "\n",
    "\n",
    "## Station Models (SM)\n",
    "* stations are in order of flow\n",
    "* X<sub>n,t</sub>\n",
    "    * X is a station flow and occupancy\n",
    "    * where n is 0..N, N=number of stations\n",
    "    * where t is 0..T, T=5 minute interval \n",
    "* O<sub>n-1..n,t-1</sub>\n",
    "    * O is on-ramps flow\n",
    "    * where n-1..n is between X<sub>n-1,t-1</sub> and X<sub>n,t-1</sub>\n",
    "* F<sub>n-1..n,t-1</sub> \n",
    "    * F is off-ramps flow\n",
    "    * where n-1..n is between X<sub>n-1,t-1</sub> and X<sub>n,t-1</sub>\n",
    "\n",
    "## Station Traffic Model (STM)\n",
    "#### STM<sub>n</sub>(X<sub>n-1,t-1</sub>, O<sub>n-1..n,t-1</sub>, F<sub>n-1..n,t-1</sub> ) = X'<sub>n,t</sub>\n",
    "* X<sub>n-1,t-1</sub> is flow and occupancy of X<sub>n-1,t-1</sub>\n",
    "* O<sub>n-1..n,t-1</sub> is flow of all OR between X<sub>n-1,t-1</sub> and X<sub>n,t</sub>\n",
    "* F<sub>n-1..n,t-1</sub> is flow of all FR between X<sub>n-1,t-1</sub> and X<sub>n,t</sub>\n",
    "* X'<sub>n,t</sub> is prediction of flow and occupancy for X'\n",
    "    \n",
    "## Freeway Traffic Model (FTM)\n",
    "### Initialization at t=0 : FTM<sub>0</sub>\n",
    "#### FTM<sub>0</sub>(STM<sub>1..N</sub>,  X<sub>0..N,0</sub>, O<sub>0..N,0</sub>, F<sub>0..N,0</sub>) = X'<sub>1..N,1</sub>\n",
    "\n",
    "* X<sub>0..N,0</sub> is initial station state\n",
    "* STM<sub>1..N</sub> is trained station traffic model\n",
    "* O<sub>0..N,0</sub> is initial on ramp flow between stations\n",
    "* F<sub>0..N,0</sub> is initial off ramp flow between stations\n",
    "* X'<sub>0..N,1</sub> is predicted traffic for X<sub>0..N</sub>\n",
    "\n",
    "### FTM at t=1..T : FTM<sub>1..T</sub>\n",
    "\n",
    "#### FTM<sub>t</sub>(STM<sub>1..N</sub>,  X'<sub>0,p;1..N,t-1</sub>, O<sub>0..N,p</sub>, F<sub>0..N,p</sub>) = X'<sub>1..N,t</sub>\n",
    "* X'<sub>1..N,t-1</sub> is previous predicted station state for station 1..0\n",
    "* X'<sub>0,p</sub> is pumped station state for station 0\n",
    "* STM<sub>0..N</sub> is trained station traffic model\n",
    "* O<sub>0..N,p</sub> is pumped on-ramp flow between stations\n",
    "* F<sub>0..N,p</sub> is pumped off-ramp flow between stations\n",
    "* X'<sub>0..N,t+1</sub> is predicted traffic for X'<sub>0..N</sub>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from datetime import time\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import trafficpassion.AnalyzeWiggles as aw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data locations\n",
    "data_5min_path = \"../data/station_5min/2015/d11/\"\n",
    "meta_path = \"../data/station_5min/2015/meta_data/d11/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get data file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get all files to process\n",
    "onlyfiles = [f for f in listdir(data_5min_path) if isfile(join(data_5min_path, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d11_text_station_5min_2015_01_01.txt.gz',\n",
       " 'd11_text_station_5min_2015_01_02.txt.gz',\n",
       " 'd11_text_station_5min_2015_01_03.txt.gz']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onlyfiles[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "colnames = [\n",
    "    'Timestamp','Station','District','Freeway','Direction_of_Travel',\n",
    "    'LaneType','StationLength','Samples',\n",
    "    'Perc_Observed','TotalFlow','AvgOccupancy','AvgSpeed',\n",
    "    'Lane1_Samples','Lane1_Flow','Lane1_AvgOcc','Lane1_AvgSpeed','Lane1_Observed',\n",
    "    'Lane2_Samples','Lane2_Flow','Lane2_AvgOcc','Lane2_AvgSpeed','Lane2_Observed',\n",
    "    'Lane3_Samples','Lane3_Flow','Lane3_AvgOcc','Lane3_AvgSpeed','Lane3_Observed',\n",
    "    'Lane4_Samples','Lane4_Flow','Lane4_AvgOcc','Lane4_AvgSpeed','Lane4_Observed',\n",
    "    'Lane5_Samples','Lane5_Flow','Lane5_AvgOcc','Lane5_AvgSpeed','Lane5_Observed',\n",
    "    'Lane6_Samples','Lane6_Flow','Lane6_AvgOcc','Lane6_AvgSpeed','Lane6_Observed',\n",
    "    'Lane7_Samples','Lane7_Flow','Lane7_AvgOcc','Lane7_AvgSpeed','Lane7_Observed',\n",
    "    'Lane8_Samples','Lane8_Flow','Lane8_AvgOcc','Lane8_AvgSpeed','Lane8_Observed'\n",
    "]\n",
    "colnames = [c.lower() for c in colnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(colnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make spark schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "struct list was generated with the following code after reading the files with inferschema = true, then hand modified \n",
    "```\n",
    "'[' + ','.join(['StructField(\"%s\",%s(),True)'% (colnames[idx], str(i.dataType))\n",
    "for idx, i in enumerate(rdd.schema)]) + ']'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print '[\\n    ' + \",\\n    \".join(['StructField(\"%s\",%s(),True)'% (colnames[idx], str(i.dataType))\n",
    "#for idx, i in enumerate(rdd.schema)]) + '\\n]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build dataframe with spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "struct_list = [\n",
    "    StructField(\"timestamp\",TimestampType(),True),\n",
    "    StructField(\"station\",IntegerType(),True),\n",
    "    StructField(\"district\",IntegerType(),True),\n",
    "    StructField(\"freeway\",IntegerType(),True),\n",
    "    StructField(\"direction_of_travel\",StringType(),True),\n",
    "    StructField(\"lanetype\",StringType(),True),\n",
    "    StructField(\"stationlength\",DoubleType(),True),\n",
    "    StructField(\"samples\",IntegerType(),True),\n",
    "    StructField(\"perc_observed\",IntegerType(),True),\n",
    "    StructField(\"totalflow\",IntegerType(),True),\n",
    "    StructField(\"avgoccupancy\",DoubleType(),True),\n",
    "    StructField(\"avgspeed\",DoubleType(),True),\n",
    "    StructField(\"lane1_samples\",IntegerType(),True),\n",
    "    StructField(\"lane1_flow\",IntegerType(),True),\n",
    "    StructField(\"lane1_avgocc\",DoubleType(),True),\n",
    "    StructField(\"lane1_avgspeed\",DoubleType(),True),\n",
    "    StructField(\"lane1_observed\",IntegerType(),True),\n",
    "    StructField(\"lane2_samples\",IntegerType(),True),\n",
    "    StructField(\"lane2_flow\",IntegerType(),True),\n",
    "    StructField(\"lane2_avgocc\",DoubleType(),True),\n",
    "    StructField(\"lane2_avgspeed\",DoubleType(),True),\n",
    "    StructField(\"lane2_observed\",IntegerType(),True),\n",
    "    StructField(\"lane3_samples\",IntegerType(),True),\n",
    "    StructField(\"lane3_flow\",IntegerType(),True),\n",
    "    StructField(\"lane3_avgocc\",DoubleType(),True),\n",
    "    StructField(\"lane3_avgspeed\",DoubleType(),True),\n",
    "    StructField(\"lane3_observed\",IntegerType(),True),\n",
    "    StructField(\"lane4_samples\",IntegerType(),True),\n",
    "    StructField(\"lane4_flow\",IntegerType(),True),\n",
    "    StructField(\"lane4_avgocc\",DoubleType(),True),\n",
    "    StructField(\"lane4_avgspeed\",DoubleType(),True),\n",
    "    StructField(\"lane4_observed\",IntegerType(),True),\n",
    "    StructField(\"lane5_samples\",IntegerType(),True),\n",
    "    StructField(\"lane5_flow\",IntegerType(),True),\n",
    "    StructField(\"lane5_avgocc\",DoubleType(),True),\n",
    "    StructField(\"lane5_avgspeed\",DoubleType(),True),\n",
    "    StructField(\"lane5_observed\",IntegerType(),True),\n",
    "    StructField(\"lane6_samples\",IntegerType(),True),\n",
    "    StructField(\"lane6_flow\",IntegerType(),True),\n",
    "    StructField(\"lane6_avgocc\",DoubleType(),True),\n",
    "    StructField(\"lane6_avgspeed\",DoubleType(),True),\n",
    "    StructField(\"lane6_observed\",IntegerType(),True),\n",
    "    StructField(\"lane7_samples\",IntegerType(),True),\n",
    "    StructField(\"lane7_flow\",IntegerType(),True),\n",
    "    StructField(\"lane7_avgocc\",DoubleType(),True),\n",
    "    StructField(\"lane7_avgspeed\",DoubleType(),True),\n",
    "    StructField(\"lane7_observed\",IntegerType(),True),\n",
    "    StructField(\"lane8_samples\",IntegerType(),True),\n",
    "    StructField(\"lane8_flow\",IntegerType(),True),\n",
    "    StructField(\"lane8_avgocc\",DoubleType(),True),\n",
    "    StructField(\"lane8_avgspeed\",DoubleType(),True),\n",
    "    StructField(\"lane8_observed\",IntegerType(),True)\n",
    "]\n",
    "\n",
    "schema_struct = StructType(struct_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(timestamp=datetime.datetime(2015, 1, 5, 0, 0), station=1100313, district=11, freeway=5, direction_of_travel=u'N', lanetype=u'FR', stationlength=None, samples=10, perc_observed=100, totalflow=6, avgoccupancy=None, avgspeed=None, lane1_samples=10, lane1_flow=6, lane1_avgocc=None, lane1_avgspeed=None, lane1_observed=1, lane2_samples=None, lane2_flow=None, lane2_avgocc=None, lane2_avgspeed=None, lane2_observed=0, lane3_samples=None, lane3_flow=None, lane3_avgocc=None, lane3_avgspeed=None, lane3_observed=0, lane4_samples=None, lane4_flow=None, lane4_avgocc=None, lane4_avgspeed=None, lane4_observed=0, lane5_samples=None, lane5_flow=None, lane5_avgocc=None, lane5_avgspeed=None, lane5_observed=0, lane6_samples=None, lane6_flow=None, lane6_avgocc=None, lane6_avgspeed=None, lane6_observed=0, lane7_samples=None, lane7_flow=None, lane7_avgocc=None, lane7_avgspeed=None, lane7_observed=0, lane8_samples=None, lane8_flow=None, lane8_avgocc=None, lane8_avgspeed=None, lane8_observed=0)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#node this is only the first 5 days of files for now\n",
    "files = [data_5min_path + filename for filename in onlyfiles[:5]]\n",
    "\n",
    "rdd = spark.read.csv(\n",
    "    files, \n",
    "    header='false',\n",
    "    timestampFormat='MM/dd/yyyy HH:mm:ss',\n",
    "    schema=schema_struct,\n",
    "    inferSchema='false'\n",
    ")\n",
    "    \n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2129760"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = rdd.select('timestamp','station','totalflow','avgoccupancy').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>station</th>\n",
       "      <th>totalflow</th>\n",
       "      <th>avgoccupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>1100313</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>1100323</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>1100326</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>1100330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>1100333</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp  station  totalflow  avgoccupancy\n",
       "0 2015-01-05  1100313        6.0           NaN\n",
       "1 2015-01-05  1100323        NaN           NaN\n",
       "2 2015-01-05  1100326        7.0           NaN\n",
       "3 2015-01-05  1100330        1.0           NaN\n",
       "4 2015-01-05  1100333       10.0           NaN"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now lets group by station, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, mean,minute, stddev, count,max as psmax,min as psmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_time = (\n",
    "    rdd.groupBy([\n",
    "        'station',\n",
    "        hour(\"timestamp\").alias(\"hour\"),\n",
    "        minute(\"timestamp\").alias(\"minute\")\n",
    "    ]).agg(\n",
    "        mean(\"totalflow\").alias(\"flow_mean\"),\n",
    "        stddev(\"totalflow\").alias(\"flow_std\"),\n",
    "        count(\"totalflow\").alias(\"flow_count\"),\n",
    "        psmax(\"totalflow\").alias(\"flow_max\"),\n",
    "        psmin(\"totalflow\").alias(\"flow_min\")\n",
    "    )\n",
    ")\n",
    "#station_time.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = station_time.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.station.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['flow_std_plus_mean'] = df.flow_mean + df.flow_std\n",
    "df['flow_std_minus_mean'] = df.flow_mean - df.flow_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df['time'] = df.apply(lambda x:time(int(x.hour),int(x.minute)),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.sort_values('time',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for g,g_df in  df.groupby('station'):\n",
    "    g_df.plot(\n",
    "        x='time', \n",
    "        y=['flow_mean','flow_std_plus_mean','flow_std_minus_mean'],\n",
    "        title='stat plots for %i'%g,\n",
    "        rot=90\n",
    "    )\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from scipy.interpolate import interp1d, Akima1DInterpolator\n",
    "from sklearn import preprocessing\n",
    "def interpolate(meanVector, kind, factor):\n",
    "    y = meanVector\n",
    "    y_len = len(y)\n",
    "    x = np.arange(0,y_len)\n",
    "    \n",
    "    interpolator = {\n",
    "        'akima': Akima1DInterpolator(x, y),\n",
    "        'cubic': interp1d(x, y, kind='cubic'),\n",
    "        'linear': interp1d(x, y, kind='linear')\n",
    "    }\n",
    "    \n",
    "    interpolate = interpolator[kind]\n",
    "\n",
    "    mid_factor = factor/2\n",
    "    \n",
    "    interp = [interpolate(np.arange(i,y_len, factor)) for i in range(factor)]\n",
    "    myArray = reduce(lambda x,y:x+y,interp)\n",
    "\n",
    "    my_x = np.arange(mid_factor,y_len, factor)\n",
    "    \n",
    "    extrapolator = {\n",
    "        'akima': Akima1DInterpolator(my_x, myArray/factor),\n",
    "        'cubic': interp1d(my_x, myArray/factor, kind='cubic'),\n",
    "        'linear': interp1d(my_x, myArray/factor, kind='linear')\n",
    "    }\n",
    "    \n",
    "    extrapolate = extrapolator[kind]\n",
    "    \n",
    "    new_x = np.arange(mid_factor,y_len-mid_factor)\n",
    "    interpolated = extrapolate( np.arange(mid_factor,y_len-mid_factor))\n",
    "\n",
    "    #wut??\n",
    "    #pad front and back with mean vector \n",
    "    xprime = np.append(np.arange(0,mid_factor), new_x)\n",
    "    xprime = np.append(xprime, np.arange(max(xprime)+1,y_len))\n",
    "    yprime = np.append(y[:mid_factor], interpolated)\n",
    "    yprime = np.append(yprime, y[-mid_factor:])\n",
    "\n",
    "    return yprime\n",
    "\n",
    "\n",
    "def smooth_vector(meanVector, kind='akima', factor=6):\n",
    "    smoothedVector = interpolate(meanVector, kind, factor )\n",
    "    diff = meanVector - smoothedVector\n",
    "    diffVector = diff/np.linalg.norm(diff) \n",
    "    \n",
    "    return {  \n",
    "        'smoothedVector': smoothedVector, \n",
    "        'diffVector': diffVector\n",
    "    }\n",
    "\n",
    "vectors = smooth_vector(g_df['flow_mean'].values, 'akima', 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectors = smooth_vector(g_df['flow_mean'].values, 'akima', 12)\n",
    "g_df['smoothedVector'] = vectors['smoothedVector']\n",
    "g_df['diffVector'] = vectors['diffVector']\n",
    "\n",
    "g_df.plot(\n",
    "    x='time',\n",
    "    y=['flow_mean','smoothedVector'], \n",
    "    rot=90,\n",
    "    title='mean vector plot for %i'%(g)\n",
    ")\n",
    "\n",
    "g_df.plot(\n",
    "    x='time',\n",
    "    y='diffVector',\n",
    "    rot=90,\n",
    "    linewidth=2,\n",
    "    title='diffVector plot for %i'%(g)\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get metadata and identify all FR OR and their preceding and antceding station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loadMeta():\n",
    "    meta_dir='../data/External/meta/2015/d11/d11_text_meta_2015_*.txt'\n",
    "    meta_files = glob.glob(meta_dir)\n",
    "\n",
    "    meta_file_list = []\n",
    "    for meta_file in meta_files:\n",
    "        date = str('_'.join(meta_file.split('_')[4:7])).split('.')[0]\n",
    "        df = pd.read_table(meta_file, index_col=None, header=0)\n",
    "        date_col = pd.Series([date] * len(df))\n",
    "        df['file_date'] = date_col\n",
    "        # drop rows that are missing latitude / longitude values\n",
    "        #df.dropna(inplace=True, subset=['Latitude', 'Longitude'], how='any')\n",
    "        meta_file_list.append(df)\n",
    "\n",
    "    meta_frame = pd.concat(meta_file_list).drop_duplicates(subset='ID', keep='last')\n",
    "\n",
    "    usefwy = [ 56, 125, 805,  52, 163,   8,  15,   5, 905,  78,  94,  54]\n",
    "\n",
    "    meta_frame = meta_frame[meta_frame.Fwy.apply(lambda x: x in usefwy)]\n",
    "\n",
    "    #Add freeway name FwyDir\n",
    "    meta_frame['freeway'] = meta_frame.Fwy.apply(str) + meta_frame.Dir\n",
    "    \n",
    "    r_c = {}\n",
    "    for c in meta_frame.columns:\n",
    "        r_c[c]=c.lower()\n",
    "    \n",
    "    meta_frame=meta_frame.rename(columns = r_c )\n",
    "    return meta_frame\n",
    "\n",
    "loadMeta().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loadMeta().head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loadMeta.type.apply(lambda x: x in [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "station_type_dict = loadMeta().loc[:,['id','type']].set_index('id').to_dict()['type']\n",
    "\n",
    "station_df = df.groupby('station')\n",
    "\n",
    "for fwy, fwy_df in loadMeta().groupby('fwy'):\n",
    "    for d, d_df in fwy_df.groupby('dir'):\n",
    "        d_df.sort_values('abs_pm', inplace=True)\n",
    "        d_df.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        ramps_index = d_df[d_df.type.apply(lambda x: x in ['FR','OR'])].index.tolist()\n",
    "        \n",
    "        b_i = [max(i-1,min(ramps_index)) for i in ramps_index]\n",
    "        b_j = [min(i+1,max(ramps_index)) for i in ramps_index]\n",
    "        \n",
    "        ramps_i = d_df.iloc[b_i,:].id\n",
    "        ramps_j = d_df.iloc[b_j,:].id\n",
    "        ramps = d_df.iloc[ramps_index,:].id\n",
    "                \n",
    "        for p in zip(ramps_i,ramps,ramps_j):\n",
    "            fig, axs = plt.subplots(figsize=[15,8], ncols=2, nrows=3, sharex=True)\n",
    "            \n",
    "            for idx, r in enumerate(p):                \n",
    "                r_df = station_df.get_group(r)\n",
    "                vectors = smooth_vector(r_df['flow_mean'].values, 'akima', 12)\n",
    "                r_df['smoothedVector'] = vectors['smoothedVector']\n",
    "                r_df['diffVector'] = vectors['diffVector']\n",
    "\n",
    "                r_df.plot(\n",
    "                    x='time',\n",
    "                    y=['flow_mean','smoothedVector'], \n",
    "                    rot=90,\n",
    "                    title='mean vector plot for %s %s %i %s'%(fwy,d,r,station_type_dict[r]),\n",
    "                    ax=axs[idx][0]\n",
    "                )\n",
    "\n",
    "                r_df.plot(\n",
    "                    x='time',\n",
    "                    y='diffVector',\n",
    "                    rot=90,\n",
    "                    linewidth=2,\n",
    "                    title='diffVector plot for %s %s %i %s'%(fwy,d,r,station_type_dict[r]),\n",
    "                    ax=axs[idx][1]\n",
    "                )\n",
    "                \n",
    "            plt.show()\n",
    "                     \n",
    "        break\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
